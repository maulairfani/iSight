{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "# Load & process\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Vector store & embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Conversations\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Post-processing\n",
    "import fitz\n",
    "\n",
    "# Token counter\n",
    "import tiktoken\n",
    "encoder = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "from langchain.callbacks.manager import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader('pdfs/2309.13963.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create embeddings from chunks\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "# Template prompt\n",
    "template = \"\"\"Based ONLY on the context below, answer the following question according to the given format!\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Format: a dictionary with keys\n",
    "    \"answer\": (answer to the question)\n",
    "    \"in-text citation\": (provide the in-text citation for the source within the context, usually in the form of [number], LEAVE IT BLANK IF NO IN-TEXT CITATION IS AVAILABLE IN THE ANSWER SOURCE)\n",
    "    \"source\" : (one sentence that serves as the source of the answer, write it exactly as it appears in the context, including new line (-\\n or \\n) if any, LEAVE IT BLANK IF NO ANSWER IS AVAILABLE IN THE CONTEXT)\n",
    "\"\"\"\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contexts_formatter(contexts):\n",
    "    result = \"\"\n",
    "    for i in range(len(contexts)):\n",
    "        result += f\"{i+1}. {contexts[i].page_content}\\n\\n\\n\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parser = SimpleJsonOutputParser()\n",
    "chain = prompt | chat_model | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former, in the context of connecting a speech encoder with a large language model (LLM) for automatic speech recognition (ASR). The study evaluates the performance of these connectors on different datasets and concludes that LLMs with Q-Formers demonstrate consistent and considerable word error rate (WER) reductions, even on out-of-domain datasets."
     ]
    }
   ],
   "source": [
    "question = \"explain the abstract of this paper\"\n",
    "contexts = docsearch.similarity_search(question, k=3)\n",
    "\n",
    "old_answer = \"\"\n",
    "len_ans = len(old_answer)\n",
    "\n",
    "for res in chain.stream({\"context\": contexts_formatter(contexts), \"question\": question, \"chat_history\": memory.buffer_as_messages}):\n",
    "    if res:\n",
    "        answer = res[\"answer\"]\n",
    "        new_answer = answer[len_ans:]\n",
    "        print(new_answer, end=\"\", flush=True)\n",
    "        old_answer = res[\"answer\"]\n",
    "        len_ans = len(old_answer)\n",
    "\n",
    "# with get_openai_callback() as cb:\n",
    "#     res_invoke = chain.invoke({\"context\": contexts_formatter(contexts), \"question\": question, \"chat_history\": memory.buffer_as_messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former, in the context of connecting a speech encoder with a large language model (LLM) for automatic speech recognition (ASR). The study evaluates the performance of these connectors on different datasets and concludes that LLMs with Q-Formers demonstrate consistent and considerable word error rate (WER) reductions, even on out-of-domain datasets.',\n",
       " 'in-text citation': '[3]',\n",
       " 'source': 'The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former.'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def dict_to_string(input_dict):\n",
    "    json_string = json.dumps(input_dict, indent=2)  # indent for pretty formatting (optional)\n",
    "    return json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex_python.converter import CurrencyRates\n",
    "\n",
    "def get_cost(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        prompt_formatted=prompt.format(context=contexts_formatter(contexts), question=question, chat_history=memory.buffer_as_messages),\n",
    "        output_from_llm=dict_to_string(res)\n",
    "):\n",
    "    curr = CurrencyRates()\n",
    "    encoder = tiktoken.encoding_for_model(model)\n",
    "    input_tokens_used = len(encoder.encode(prompt_formatted)) + 7 # Jaga-jaga\n",
    "    output_tokens_used = len(encoder.encode(output_from_llm))\n",
    "    total_token = input_tokens_used + output_tokens_used\n",
    "\n",
    "    input_price = round((0.0015/1000) * input_tokens_used, 8)\n",
    "    output_price = round((0.002/1000) * output_tokens_used, 8)\n",
    "    total_price_usd = round(input_price + output_price, 8)\n",
    "    total_price_idr = curr.convert('USD', 'IDR', total_price_usd)\n",
    "\n",
    "\n",
    "    return f\"\"\"Tokens Used: {total_token}\n",
    "        Prompt Tokens: {input_tokens_used}\n",
    "        Completion Tokens: {output_tokens_used}\n",
    "    Total Cost (USD): ${total_price_usd}\n",
    "    Total Cost (IDR): Rp{total_price_idr}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlighting Sources\n",
    "def get_matches(source, contexts=contexts, k=3):\n",
    "    for i in range(k):\n",
    "        idx_awal = contexts[i].page_content.find(source)\n",
    "        if idx_awal != -1:\n",
    "            idx_akhir = contexts[i].page_content[idx_awal:].find(\".\")\n",
    "            idx_akhir += idx_awal\n",
    "        \n",
    "            match_ = contexts[i].page_content[idx_awal:idx_akhir]\n",
    "            if len(match_) > 10:\n",
    "                page_num = contexts[i].metadata[\"page\"]\n",
    "                return contexts[i].page_content[idx_awal:idx_akhir], page_num\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def highlight_pdf(path, match_, page_num, output_path):\n",
    "    pdf = fitz.open(path)\n",
    "    page = pdf[page_num]\n",
    "\n",
    "    matches = page.search_for(match_.replace(\"-\\n\", \"\"))\n",
    "\n",
    "    for m in matches:\n",
    "        page.add_highlight_annot(m)\n",
    "\n",
    "    pdf.save(output_path)\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get references\n",
    "def get_reference(docs, in_text_citation):\n",
    "    model = OpenAI(openai_api_key=\"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\")\n",
    "\n",
    "    get_citation_template = \"\"\"From the reference list below, rewrite the specified references!\n",
    "    References:\n",
    "    {references}\n",
    "\n",
    "    Please rewrite the references related to the following numbers:\n",
    "    {in_text_citation}\n",
    "    \"\"\"\n",
    "\n",
    "    GET_CITATION_PROMPT = PromptTemplate.from_template(get_citation_template)\n",
    "\n",
    "    get_reference_chain = chain = GET_CITATION_PROMPT | model\n",
    "\n",
    "    for page_number in range(len(docs)):\n",
    "        page = docs[page_number]\n",
    "        text = page.page_content\n",
    "\n",
    "        if \"References\" in text or \"REFERENCES\" in text:\n",
    "            references_text = text.split(\"References\")[1] if \"References\" in text else text.split(\"REFERENCES\")[1]\n",
    "\n",
    "    result = get_reference_chain.invoke({\"references\": references_text, \"in_text_citation\":in_text_citation})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if res[\"source\"]:\n",
    "    match_, page_num = get_matches(res[\"source\"][:15], contexts)\n",
    "\n",
    "    if match_:\n",
    "        highlight_pdf(\"pdfs/2309.13963.pdf\", match_, page_num, \"pdfs/highlighted/high_pdf.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] Anil, R., Dai, A.M., Firat, O., et al., “PaLM 2 technical report,” arXiv:2305.10403, 2023.\n"
     ]
    }
   ],
   "source": [
    "if res[\"in-text citation\"]:\n",
    "    ref_result = get_reference(docs, res[\"in-text citation\"]).strip()\n",
    "print(ref_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maula\\Documents\\Project\\uas_nlp\\notebook_v4.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_nlp/notebook_v4.ipynb#Y131sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_nlp/notebook_v4.ipynb#Y131sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# openai.api_key = OPENAI_API_KEY\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_nlp/notebook_v4.ipynb#Y131sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m llm_direct \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_nlp/notebook_v4.ipynb#Y131sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPT-3.5-Turbo\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_nlp/notebook_v4.ipynb#Y131sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             messages\u001b[39m=\u001b[39m[{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mHalo!\u001b[39m\u001b[39m\"\u001b[39m}],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_nlp/notebook_v4.ipynb#Y131sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             temperature\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_nlp/notebook_v4.ipynb#Y131sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             max_tokens\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_nlp/notebook_v4.ipynb#Y131sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             stream \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_nlp/notebook_v4.ipynb#Y131sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_nlp/notebook_v4.ipynb#Y131sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_nlp/notebook_v4.ipynb#Y131sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Works well -- Looping over the response for \"streaming effect\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maula\\Documents\\Project\\uas_nlp\\chatbot\\lib\\site-packages\\openai\\_utils\\_proxy.py:22\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, attr: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_proxied__(), attr)\n",
      "File \u001b[1;32mc:\\Users\\maula\\Documents\\Project\\uas_nlp\\chatbot\\lib\\site-packages\\openai\\_utils\\_proxy.py:43\u001b[0m, in \u001b[0;36mLazyProxy.__get_proxied__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get_proxied__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_cache:\n\u001b[1;32m---> 43\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load__()\n\u001b[0;32m     45\u001b[0m     proxied \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__proxied\n\u001b[0;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m proxied \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maula\\Documents\\Project\\uas_nlp\\chatbot\\lib\\site-packages\\openai\\lib\\_old_api.py:33\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__load__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__load__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "llm_direct = openai.ChatCompletion.create(\n",
    "            model=\"GPT-3.5-Turbo\", \n",
    "            messages=[{\"role\": \"user\", \"content\": \"Halo!\"}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=50,\n",
    "            stream = True,\n",
    "        )\n",
    "\n",
    "tokens = []\n",
    "# Works well -- Looping over the response for \"streaming effect\"\n",
    "for resp in llm_direct:\n",
    "    if resp.get(\"choices\") and resp[\"choices\"][0].get(\"delta\") and resp[\"choices\"][0][\"delta\"].get(\"content\"):\n",
    "        tokens.append( resp[\"choices\"][0][\"delta\"][\"content\"] )\n",
    "        result = \"\".join(tokens)\n",
    "        # st.write(result) \n",
    "        print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
