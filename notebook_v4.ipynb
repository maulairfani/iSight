{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "# Load & process\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Vector store & embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Conversations\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Post-processing\n",
    "import fitz\n",
    "\n",
    "# Token counter\n",
    "import tiktoken\n",
    "encoder = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "from langchain.callbacks.manager import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader('pdfs/2309.13963.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create embeddings from chunks\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "# Template prompt\n",
    "template = \"\"\"Based ONLY on the context below, answer the following question according to the given format!\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Format: a dictionary with keys\n",
    "    \"answer\": (answer to the question)\n",
    "    \"in-text citation\": (provide the in-text citation for the source within the context, usually in the form of [number], LEAVE IT BLANK IF NO IN-TEXT CITATION IS AVAILABLE IN THE ANSWER SOURCE)\n",
    "    \"source\" : (one sentence that serves as the source of the answer, write it exactly as it appears in the context, including new line (-\\n or \\n) if any, LEAVE IT BLANK IF NO ANSWER IS AVAILABLE IN THE CONTEXT)\n",
    "\"\"\"\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contexts_formatter(contexts):\n",
    "    result = \"\"\n",
    "    for i in range(len(contexts)):\n",
    "        result += f\"{i+1}. {contexts[i].page_content}\\n\\n\\n\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parser = SimpleJsonOutputParser()\n",
    "chain = prompt | chat_model | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Former is a Transformer-based module that converts variable-length input sequences into fixed-length output query representations."
     ]
    }
   ],
   "source": [
    "question = \"jelasin apa itu q-former\"\n",
    "contexts = docsearch.similarity_search(question, k=3)\n",
    "\n",
    "old_answer = \"\"\n",
    "len_ans = len(old_answer)\n",
    "\n",
    "for res in chain.stream({\"context\": contexts_formatter(contexts), \"question\": question, \"chat_history\": memory.buffer_as_messages}):\n",
    "    if res:\n",
    "        answer = res[\"answer\"]\n",
    "        new_answer = answer[len_ans:]\n",
    "        print(new_answer, end=\"\", flush=True)\n",
    "        old_answer = res[\"answer\"]\n",
    "        len_ans = len(old_answer)\n",
    "\n",
    "# with get_openai_callback() as cb:\n",
    "#     res_invoke = chain.invoke({\"context\": contexts_formatter(contexts), \"question\": question, \"chat_history\": memory.buffer_as_messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Q-Former is a Transformer-based module that converts variable-length input sequences into fixed-length output query representations.',\n",
       " 'in-text citation': '[3.3]',\n",
       " 'source': 'Q-Former [20] is a Transformer-based module converting variable-length input sequences into fixed-length output query representations.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def dict_to_string(input_dict):\n",
    "    json_string = json.dumps(input_dict, indent=2)  # indent for pretty formatting (optional)\n",
    "    return json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex_python.converter import CurrencyRates\n",
    "\n",
    "def get_cost(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        prompt_formatted=prompt.format(context=contexts_formatter(contexts), question=question, chat_history=memory.buffer_as_messages),\n",
    "        output_from_llm=dict_to_string(res)\n",
    "):\n",
    "    curr = CurrencyRates()\n",
    "    encoder = tiktoken.encoding_for_model(model)\n",
    "    input_tokens_used = len(encoder.encode(prompt_formatted)) + 7 # Jaga-jaga\n",
    "    output_tokens_used = len(encoder.encode(output_from_llm))\n",
    "    total_token = input_tokens_used + output_tokens_used\n",
    "\n",
    "    input_price = round((0.0015/1000) * input_tokens_used, 8)\n",
    "    output_price = round((0.002/1000) * output_tokens_used, 8)\n",
    "    total_price_usd = round(input_price + output_price, 8)\n",
    "    total_price_idr = curr.convert('USD', 'IDR', total_price_usd)\n",
    "\n",
    "\n",
    "    return f\"\"\"Tokens Used: {total_token}\n",
    "        Prompt Tokens: {input_tokens_used}\n",
    "        Completion Tokens: {output_tokens_used}\n",
    "    Total Cost (USD): ${total_price_usd}\n",
    "    Total Cost (IDR): Rp{total_price_idr}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlighting Sources\n",
    "def get_matches(source, contexts=contexts, k=3):\n",
    "    for i in range(k):\n",
    "        idx_awal = contexts[i].page_content.find(source)\n",
    "        if idx_awal != -1:\n",
    "            idx_akhir = contexts[i].page_content[idx_awal:].find(\".\")\n",
    "            idx_akhir += idx_awal\n",
    "        \n",
    "            match_ = contexts[i].page_content[idx_awal:idx_akhir]\n",
    "            if len(match_) > 10:\n",
    "                page_num = contexts[i].metadata[\"page\"]\n",
    "                return contexts[i].page_content[idx_awal:idx_akhir], page_num\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def highlight_pdf(path, match_, page_num, output_path):\n",
    "    pdf = fitz.open(path)\n",
    "    page = pdf[page_num]\n",
    "\n",
    "    matches = page.search_for(match_.replace(\"-\\n\", \"\"))\n",
    "\n",
    "    for m in matches:\n",
    "        page.add_highlight_annot(m)\n",
    "\n",
    "    pdf.save(output_path)\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get references\n",
    "def get_reference(docs, in_text_citation):\n",
    "    model = OpenAI()\n",
    "\n",
    "    get_citation_template = \"\"\"From the reference list below, rewrite the specified references!\n",
    "    References:\n",
    "    {references}\n",
    "\n",
    "    Please rewrite the references related to the following numbers:\n",
    "    {in_text_citation}\n",
    "    \"\"\"\n",
    "\n",
    "    GET_CITATION_PROMPT = PromptTemplate.from_template(get_citation_template)\n",
    "\n",
    "    get_reference_chain = chain = GET_CITATION_PROMPT | model\n",
    "\n",
    "    for page_number in range(len(docs)):\n",
    "        page = docs[page_number]\n",
    "        text = page.page_content\n",
    "\n",
    "        if \"References\" in text or \"REFERENCES\" in text:\n",
    "            references_text = text.split(\"References\")[1] if \"References\" in text else text.split(\"REFERENCES\")[1]\n",
    "\n",
    "    result = get_reference_chain.invoke({\"references\": references_text, \"in_text_citation\":in_text_citation})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if res[\"source\"]:\n",
    "    match_, page_num = get_matches(res[\"source\"][:15], contexts)\n",
    "\n",
    "    if match_:\n",
    "        highlight_pdf(\"pdfs/2309.13963.pdf\", match_, page_num, \"pdfs/highlighted/high_pdf.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] R. Anil, A.M. Dai, O. Firat, et al., “PaLM 2 technical report,” arXiv:2305.10403, 2023.\n",
      "[23] Y . Zhang, W. Han, et al., “Google USM: Scaling automatic speech recognition beyond 100 languages,” arXiv:2303.01037, 2023.\n"
     ]
    }
   ],
   "source": [
    "if res[\"in-text citation\"]:\n",
    "    ref_result = get_reference(docs, res[\"in-text citation\"]).strip()\n",
    "print(ref_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
