{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "# Load & process\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Vector store & embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Conversations\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Post-processing\n",
    "import fitz\n",
    "\n",
    "# Token counter\n",
    "import tiktoken\n",
    "encoder = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "from langchain.callbacks.manager import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELETE_THIS = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(model_name='gpt-3.5-turbo-16k',\n",
    "               temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader('pdfs/2309.13963.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create embeddings from chunks\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "chat_model = ChatOpenAI()\n",
    "llm = OpenAI()\n",
    "\n",
    "# Template prompt\n",
    "template = \"\"\"Based ONLY on the context below, answer the following question!\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Remember to answer it ONLY from the given context!\n",
    "\"\"\"\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contexts_formatter(contexts):\n",
    "    result = \"\"\n",
    "    for i in range(len(contexts)):\n",
    "        result += f\"{i+1}. {contexts[i].page_content}\\n\\n\\n\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "question = \"what is q-former\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Specifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the\\nencoder outputs in the standard Transformer.\\n4. SEGMENT-LEVEL Q-FORMER\\nTransformer-based speech encoders can have limitations on the in-\\nput sequence duration [18]. To enable LLMs to process with longer', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'})]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vanilla\n",
    "\n",
    "contexts_vanilla = docsearch.similarity_search(question, k=1)\n",
    "contexts_vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='6.3. Trainable queries of Q-Former\\nRecapping Section 3.3, the number of trainable queries used in the\\nQ-Former determines the number of its output tokens. This sec-\\ntion compares Q-Formers with different numbers of queries. Table\\n3 shows that increasing the number of queries to 80 can consider-\\nably reduce WERs, which implies that using ∼80 tokens can retain', metadata={'doc_id': '94811155-4fb0-4ca7-9cef-52a551d1e25c', 'page': 2, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='Specifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the\\nencoder outputs in the standard Transformer.\\n4. SEGMENT-LEVEL Q-FORMER\\nTransformer-based speech encoders can have limitations on the in-\\nput sequence duration [18]. To enable LLMs to process with longer', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='V oice, and GigaSpeech datasets, where the LLMs with Q-Formers\\ndemonstrated consistent and considerable word error rate (WER)\\nreductions over LLMs with other connector structures. Q-Former-\\nbased LLMs can generalise well to out-of-domain datasets, where\\n12% relative WER reductions over the Whisper baseline ASR model\\nwere achieved on the Eval2000 test set without using any in-domain', metadata={'doc_id': '4b71757e-6462-4b0a-9870-80b5978cc314', 'page': 0, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='Tspeech=MultiHead (H,E,E). (3)3.3. Q-Former\\nQ-Former [20] is a Transformer-based module converting variable-\\nlength input sequences into fixed-length output query representa-\\ntions. It was initially proposed for visual-text modality alignment,\\nand here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='and here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-\\nturesXthrough multi-head self-attention and cross-attention layers,\\nSpecifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'})]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiquery Retriever\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=docsearch.as_retriever(), llm=llm\n",
    ")\n",
    "contexts_multiquery = retriever_from_llm.get_relevant_documents(query=question, k=1)\n",
    "contexts_multiquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the\\nencoder outputs in the standard Transformer.', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed.', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='Q-Former results in the lowest WERs on both test sets by producing only 80 output tokens. Although fully connected layers with 300 output tokens (with m= 5 in Sec. 3.1) can achieve similar WERs to Q-Former, it requires much more calculations and memory usage.', metadata={'doc_id': '94811155-4fb0-4ca7-9cef-52a551d1e25c', 'page': 2, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain.', metadata={'doc_id': '4b71757e-6462-4b0a-9870-80b5978cc314', 'page': 0, 'source': 'pdfs/2309.13963.pdf'})]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contextual Compression\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=docsearch.as_retriever())\n",
    "contexts_compression = compression_retriever.get_relevant_documents(question, k=1)\n",
    "contexts_compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Tspeech=MultiHead (H,E,E). (3)3.3. Q-Former\\nQ-Former [20] is a Transformer-based module converting variable-\\nlength input sequences into fixed-length output query representa-\\ntions. It was initially proposed for visual-text modality alignment,\\nand here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='and here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-\\nturesXthrough multi-head self-attention and cross-attention layers,\\nSpecifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='training data from Switchboard. Moreover, a novel segment-level\\nQ-Former is proposed to enable LLMs to recognise speech seg-\\nments with a duration exceeding the limitation of the encoders,\\nwhich results in 17% relative WER reductions over other connector\\nstructures on 90-second-long speech data.\\nIndex Terms —Large language model, automatic speech recog-\\nnition, Q-Former, long-form speech', metadata={'doc_id': '4b71757e-6462-4b0a-9870-80b5978cc314', 'page': 0, 'source': 'pdfs/2309.13963.pdf'})]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "import lark\n",
    "\n",
    "document_content_description = \"Sebuah researh paper\"\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"page number\",\n",
    "        type=\"integer\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"file path\",\n",
    "        type=\"string\"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    docsearch,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    enable_limit=True,\n",
    "\n",
    ")\n",
    "\n",
    "contexts_selfquery = retriever.get_relevant_documents(\"3 dokumen tentang Q-Former\", k=1)\n",
    "contexts_selfquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Specifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the\\nencoder outputs in the standard Transformer.\\n4. SEGMENT-LEVEL Q-FORMER\\nTransformer-based speech encoders can have limitations on the in-\\nput sequence duration [18]. To enable LLMs to process with longer', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='and here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-\\nturesXthrough multi-head self-attention and cross-attention layers,\\nSpecifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='multi-head cross-attention and Q-Former, are compared. Q-Former\\nresults in the lowest WERs on both test sets by producing only 80\\noutput tokens. Although fully connected layers with 300 output to-\\nkens (with m= 5 in Sec. 3.1) can achieve similar WERs to Q-\\nFormer, it requires much more calculations and memory usage. The\\nWERs produced by the fully connected layers connector with 75', metadata={'doc_id': '94811155-4fb0-4ca7-9cef-52a551d1e25c', 'page': 2, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='V oice, and GigaSpeech datasets, where the LLMs with Q-Formers\\ndemonstrated consistent and considerable word error rate (WER)\\nreductions over LLMs with other connector structures. Q-Former-\\nbased LLMs can generalise well to out-of-domain datasets, where\\n12% relative WER reductions over the Whisper baseline ASR model\\nwere achieved on the Eval2000 test set without using any in-domain', metadata={'doc_id': '4b71757e-6462-4b0a-9870-80b5978cc314', 'page': 0, 'source': 'pdfs/2309.13963.pdf'})]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = docsearch.as_retriever(type=\"mmr\")\n",
    "retriever.get_relevant_documents(question, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=docsearch,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Specifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the\\nencoder outputs in the standard Transformer.\\n4. SEGMENT-LEVEL Q-FORMER\\nTransformer-based speech encoders can have limitations on the in-\\nput sequence duration [18]. To enable LLMs to process with longer', metadata={'doc_id': '11526dca-9631-4c4c-98a6-42a8ecd2e2f3', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='Specifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the\\nencoder outputs in the standard Transformer.\\n4. SEGMENT-LEVEL Q-FORMER\\nTransformer-based speech encoders can have limitations on the in-\\nput sequence duration [18]. To enable LLMs to process with longer', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='and here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-\\nturesXthrough multi-head self-attention and cross-attention layers,\\nSpecifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the', metadata={'doc_id': 'da9c286e-7d50-4809-91d5-0d8ed5a22bac', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='and here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-\\nturesXthrough multi-head self-attention and cross-attention layers,\\nSpecifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the', metadata={'doc_id': '11526dca-9631-4c4c-98a6-42a8ecd2e2f3', 'page': 1, 'source': 'pdfs/2309.13963.pdf'})]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch.similarity_search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3. MODULE CONNECTOR\\nAs shown in Fig. 1, the proposed ASR model consists of three mod-\\nules: a frozen speech encoder, a trainable module connector and\\na frozen LLM. This section introduces three connectors, including\\nfully connected layers, multi-head cross-attention and Q-Former.\\n❄ Speech EncoderLinearMHCA\\n❄ Large Language Model(a)(b)(c)Transcription\\nQ-FormerQ-Former queryVicuna embeddingConv kernel\\nLinear\\nFig. 1 . Illustration of integrating a speech encoder and an LLM into\\nan ASR system with a module connector of: (a) fully connected\\nlayers, (b) multi-head cross-attention, and (c) Q-Former.\\nFor clarity, some basic notations are defined as follows: X∈\\nRnx×dxdenotes the speech features obtained from the speech en-\\ncoder, and the module connector compresses XintoTspeech∈\\nRnt×dtwhich are input to the LLM to produce ASR transcriptions.\\nH∈Rnh×dhdenotes the hidden states in connectors while nandd\\nare the numbers of vectors and hidden dimensions respectively.\\n3.1. Fully connected layers\\nTo compress the length of speech features, madjacent frames xi,\\nxi+1, ...,xi+m−1are stacked into hi∈Rm×dx. Then two Linear (·)\\nlayers with ReLU (·)in between are introduced as follows:\\nTspeech=Linear (ReLU (Linear (H))), (1)\\nwhere Hconsists of hiof a batch of samples. Actually, the vector\\nstacking operation together with the first linear layer works the same\\nas a 1-dimensional (-d) convolutional layer, Conv1d (·).\\n3.2. Multi-head cross-attention\\nTo bridge the gap between the multi-modal encoder output features\\nXand LLM input textual features Tspeech, a multi-head attention\\nlayer [33] denoted as MultiHead (Query ,Key,Value )is used in the\\nmulti-head cross-attention approach to align the two feature spaces\\n[19]. First, a Conv1d (·)layer reduces the length of the speech input\\nby a rate of s. Then the hidden states Hare converted to Tspeech\\nbased on the textual embeddings Eusing MultiHead (·). That is,\\nH=Linear (Conv1d (X)) (2)\\nTspeech=MultiHead (H,E,E). (3)3.3. Q-Former\\nQ-Former [20] is a Transformer-based module converting variable-\\nlength input sequences into fixed-length output query representa-\\ntions. It was initially proposed for visual-text modality alignment,\\nand here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-\\nturesXthrough multi-head self-attention and cross-attention layers,\\nSpecifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the\\nencoder outputs in the standard Transformer.\\n4. SEGMENT-LEVEL Q-FORMER\\nTransformer-based speech encoders can have limitations on the in-\\nput sequence duration [18]. To enable LLMs to process with longer\\nspeech inputs, the whole sequence can be split into several shorter\\nsegments to transform by the speech encoder separately. Such seg-\\nments can be concatenated to reform a single sequence at either\\nthe input or output end of Q-Former. In this paper, the structure\\nof segment-level Q-Former (seg-QF) shown in Fig. 2 is proposed,\\nwhich uses a Q-Former to transform each encoder output segment\\nsimultaneously and concatenates their fixed-length output token se-\\nquences before feeding into the LLM. Compared to performing the\\nconcatenation at the Q-Former input end and producing a fixed num-\\nber of nqoutput tokens, seg-QF allows varying the number of out-\\nput tokens N×nqaccording to the number of segments N, which\\nis more suitable for speech inputs with variable lengths in a wide\\nrange. Note the trainable query embeddings Qand Q-Former lay-\\ners are shared among all the segments, and seg-QF can be initialised\\nwith a pre-trained standard Q-Former.\\n❄ Speech Encoder12Seg-QF\\n❄ Large Language ModelTranscription\\n………pos:pos:\\nFig. 2 . The model structure of segment-level Q-Former (seg-QF).\\nThe integers in rectangles are segment-level positional encodings.\\nDespite that relative positions of the frames are provided by the\\nspeech encoder within each segment Si∈Rnx×dx, Seg-QF is not\\naware of their absolute positions in the whole input sequence. To\\ninform Seg-QF with such information, segment-level position em-\\nbeddings pi∈Rdxare added to X, as shown in Fig. 2. Specifically,\\nTspeech= [QF(Q,Si⊕pi)]N\\ni=1, (4)\\nwhere Si⊕qimeans adding qito each row of Si.', metadata={'source': 'pdfs/2309.13963.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = \"\"\n",
    "# for res in chain.stream({\"context\": contexts_formatter(contexts), \"question\": question, \"chat_history\": memory.buffer_as_messages}):\n",
    "#     if res:\n",
    "#         print(res.content, end=\"\", flush=True)\n",
    "#         answer += res.content\n",
    "\n",
    "# # with get_openai_callback() as cb:\n",
    "# #     res_invoke = chain.invoke({\"context\": contexts_formatter(contexts), \"question\": question, \"chat_history\": memory.buffer_as_messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Former, denoted as QF (Q,X), in this context refers to a specific model consisting of two Transformer decoder blocks with the causal attention masks removed. It uses Q as the decoder inputs and X as the encoder outputs in the standard Transformer.Q-Former is a Transformer-based module that converts variable-length input sequences into fixed-length output query representations. It was initially proposed for visual-text modality alignment and is applied to audio-text alignment in this context. Q-Former consists of two Transformer decoder blocks with the causal attention masks removed. It uses trainable query embeddings Q to interact with the input features X through multi-head self-attention and cross-attention layers. The number of trainable queries used in the Q-Former determines the number of its output tokens. Increasing the number of queries to 80 can considerably reduce word error rates (WERs).Based on the given context, Q-Former refers to a model or system that consists of two Transformer decoder blocks with the causal attention masks removed. It is used as the decoder inputs (Q) and the encoder outputs (X) in the standard Transformer. Q-Former has been shown to achieve low Word Error Rates (WERs) and can generalize well to out-of-domain datasets.Q-Former is a Transformer-based module that converts variable-length input sequences into fixed-length output query representations. It was initially proposed for visual-text modality alignment and is applied here to audio-text alignment. In each Q-Former block, trainable query embeddings Q interact with the input features X through multi-head self-attention and cross-attention layers. Q-Former consists of two Transformer decoder blocks with the causal attention masks removed. It is used as the decoder inputs for speech recognition tasks and has shown improvements in recognition accuracy for long-form speech data."
     ]
    }
   ],
   "source": [
    "# contexts_vanilla\n",
    "# contexts_multiquery\n",
    "# contexts_compression\n",
    "# contexts_selfquery\n",
    "\n",
    "answer_vanilla = \"\"\n",
    "answer_multiquery = \"\"\n",
    "answer_compression = \"\"\n",
    "answer_selfquery = \"\"\n",
    "\n",
    "# VANILLA\n",
    "for res in chain.stream({\"context\": contexts_formatter(contexts_vanilla), \"question\": question, \"chat_history\": memory.buffer_as_messages}):\n",
    "    if res:\n",
    "        print(res.content, end=\"\", flush=True)\n",
    "        answer_vanilla += res.content\n",
    "\n",
    "# MULTIQUERY\n",
    "for res in chain.stream({\"context\": contexts_formatter(contexts_multiquery), \"question\": question, \"chat_history\": memory.buffer_as_messages}):\n",
    "    if res:\n",
    "        print(res.content, end=\"\", flush=True)\n",
    "        answer_multiquery += res.content\n",
    "\n",
    "# COMPRESSION\n",
    "for res in chain.stream({\"context\": contexts_formatter(contexts_compression), \"question\": question, \"chat_history\": memory.buffer_as_messages}):\n",
    "    if res:\n",
    "        print(res.content, end=\"\", flush=True)\n",
    "        answer_compression += res.content\n",
    "\n",
    "\n",
    "# SELFQUERY\n",
    "for res in chain.stream({\"context\": contexts_formatter(contexts_selfquery), \"question\": question, \"chat_history\": memory.buffer_as_messages}):\n",
    "    if res:\n",
    "        print(res.content, end=\"\", flush=True)\n",
    "        answer_selfquery += res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Vanilla--\n",
      " Q-Former, denoted as QF (Q,X), in this context refers to a specific model consisting of two Transformer decoder blocks with the causal attention masks removed. It uses Q as the decoder inputs and X as the encoder outputs in the standard Transformer.\n",
      "\n",
      "--Multiquery--\n",
      " Q-Former is a Transformer-based module that converts variable-length input sequences into fixed-length output query representations. It was initially proposed for visual-text modality alignment and is applied to audio-text alignment in this context. Q-Former consists of two Transformer decoder blocks with the causal attention masks removed. It uses trainable query embeddings Q to interact with the input features X through multi-head self-attention and cross-attention layers. The number of trainable queries used in the Q-Former determines the number of its output tokens. Increasing the number of queries to 80 can considerably reduce word error rates (WERs).\n",
      "\n",
      "--Compression--\n",
      " Based on the given context, Q-Former refers to a model or system that consists of two Transformer decoder blocks with the causal attention masks removed. It is used as the decoder inputs (Q) and the encoder outputs (X) in the standard Transformer. Q-Former has been shown to achieve low Word Error Rates (WERs) and can generalize well to out-of-domain datasets.\n",
      "\n",
      "--SelfQuery--\n",
      " Q-Former is a Transformer-based module that converts variable-length input sequences into fixed-length output query representations. It was initially proposed for visual-text modality alignment and is applied here to audio-text alignment. In each Q-Former block, trainable query embeddings Q interact with the input features X through multi-head self-attention and cross-attention layers. Q-Former consists of two Transformer decoder blocks with the causal attention masks removed. It is used as the decoder inputs for speech recognition tasks and has shown improvements in recognition accuracy for long-form speech data.\n"
     ]
    }
   ],
   "source": [
    "print(\"--Vanilla--\\n\",answer_vanilla)\n",
    "print(\"\\n--Multiquery--\\n\",answer_multiquery)\n",
    "print(\"\\n--Compression--\\n\",answer_compression)\n",
    "print(\"\\n--SelfQuery--\\n\",answer_selfquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import faithfulness, answer_relevancy, context_relevancy, context_recall\n",
    "from ragas.langchain import RagasEvaluatorChain\n",
    "\n",
    "\n",
    "# make eval chains\n",
    "eval_retrieve = {\n",
    "    m.name: RagasEvaluatorChain(metric=m) \n",
    "    for m in [context_relevancy]\n",
    "}\n",
    "\n",
    "eval_generate = {\n",
    "    n.name: RagasEvaluatorChain(metric=n) \n",
    "    for n in [faithfulness, answer_relevancy]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dictionary\n",
    "vanilla = {'source_documents': contexts_vanilla, 'query': question, 'result': answer_vanilla}\n",
    "multiquery = {'source_documents': contexts_multiquery, 'query': question, 'result': answer_multiquery}\n",
    "compression = {'source_documents': contexts_compression, 'query': question, 'result': answer_compression}\n",
    "selfquery = {'source_documents': contexts_selfquery, 'query': question, 'result': answer_selfquery}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VANILLA context_relevancy_score: 0.0\n",
      "MULTIQUERY context_relevancy_score: 0.023255813953488372\n",
      "COMPRESSIONcontext_relevancy_score: 0.4166666666666667\n",
      "SELFQUERYcontext_relevancy_score: 0.11538461538461539\n"
     ]
    }
   ],
   "source": [
    "eval_retrieve = {\n",
    "    m.name: RagasEvaluatorChain(metric=m) \n",
    "    for m in [context_relevancy]\n",
    "}\n",
    "\n",
    "for name, eval_retrieve in eval_retrieve.items():   \n",
    "    score_name = f\"{name}_score\"\n",
    "    print(f\"VANILLA {score_name}: {eval_retrieve(vanilla)[score_name]}\")\n",
    "\n",
    "eval_retrieve = {\n",
    "    m.name: RagasEvaluatorChain(metric=m) \n",
    "    for m in [context_relevancy]\n",
    "}\n",
    "    \n",
    "for name, eval_retrieve in eval_retrieve.items():   \n",
    "    score_name = f\"{name}_score\"\n",
    "    print(f\"MULTIQUERY {score_name}: {eval_retrieve(multiquery)[score_name]}\") \n",
    "\n",
    "eval_retrieve = {\n",
    "    m.name: RagasEvaluatorChain(metric=m) \n",
    "    for m in [context_relevancy]\n",
    "}\n",
    "\n",
    "\n",
    "for name, eval_retrieve in eval_retrieve.items():   \n",
    "    score_name = f\"{name}_score\"\n",
    "    print(f\"COMPRESSION{score_name}: {eval_retrieve(compression)[score_name]}\")\n",
    "\n",
    "\n",
    "eval_retrieve = {\n",
    "    m.name: RagasEvaluatorChain(metric=m) \n",
    "    for m in [context_relevancy]\n",
    "}\n",
    "\n",
    "for name, eval_retrieve in eval_retrieve.items():   \n",
    "    score_name = f\"{name}_score\"\n",
    "    print(f\"SELFQUERY{score_name}: {eval_retrieve(selfquery)[score_name]}\")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VANILLA faithfulness_score: 1.0\n",
      "VANILLA answer_relevancy_score: 0.8682710544670492\n",
      "MULTIQUERY faithfulness_score: 1.0\n",
      "MULTIQUERY answer_relevancy_score: 0.8624036284465717\n",
      "COMPRESSIONfaithfulness_score: 1.0\n",
      "COMPRESSIONanswer_relevancy_score: 0.8320173329152141\n",
      "SELFQUERYfaithfulness_score: 1.0\n",
      "SELFQUERYanswer_relevancy_score: 0.8169073597617537\n"
     ]
    }
   ],
   "source": [
    "eval_generate = {\n",
    "    n.name: RagasEvaluatorChain(metric=n) \n",
    "    for n in [faithfulness, answer_relevancy]\n",
    "\n",
    "}\n",
    "for name, eval_generate in eval_generate.items():   \n",
    "    score_name = f\"{name}_score\"\n",
    "    print(f\"VANILLA {score_name}: {eval_generate(vanilla)[score_name]}\")\n",
    "\n",
    "\n",
    "eval_generate = {\n",
    "    n.name: RagasEvaluatorChain(metric=n) \n",
    "    for n in [faithfulness, answer_relevancy]\n",
    "\n",
    "}\n",
    "    \n",
    "for name, eval_generate in eval_generate.items():   \n",
    "    score_name = f\"{name}_score\"\n",
    "    print(f\"MULTIQUERY {score_name}: {eval_generate(multiquery)[score_name]}\") \n",
    "\n",
    "eval_generate = {\n",
    "    n.name: RagasEvaluatorChain(metric=n) \n",
    "    for n in [faithfulness, answer_relevancy]\n",
    "\n",
    "}\n",
    "\n",
    "for name, eval_generate in eval_generate.items():   \n",
    "    score_name = f\"{name}_score\"\n",
    "    print(f\"COMPRESSION {score_name}: {eval_generate(compression)[score_name]}\")\n",
    "\n",
    "\n",
    "eval_generate = {\n",
    "    n.name: RagasEvaluatorChain(metric=n) \n",
    "    for n in [faithfulness, answer_relevancy]\n",
    "\n",
    "}\n",
    "\n",
    "for name, eval_generate in eval_generate.items():   \n",
    "    score_name = f\"{name}_score\"\n",
    "    print(f\"SELFQUERY {score_name}: {eval_generate(selfquery)[score_name]}\")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template prompt\n",
    "template_reference = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Based on the question and answer pair above, find where the answer originates from within the given context.\n",
    "\n",
    "Return a JSON object with the following keys:\n",
    "    `page`: (the value is in the form of a list of page numbers, can be more than one page)\n",
    "    `source`: (the value is in the form of a list of sentences used as a reference for the answer, write exactly as it appears in the context including the in-text citation, using the language used in the context)\n",
    "    `in-text citation` : (list of in-text citation appears in contexts used as reference for the answer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_reference = PromptTemplate.from_template(template_reference)\n",
    "\n",
    "# Output parser\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "# LCEL\n",
    "reference_chain = prompt_reference | llm | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = reference_chain.invoke({\n",
    "    \"context\": contexts_formatter(contexts), \n",
    "    \"question\": question, \n",
    "    \"answer\": answer\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "\n",
    "matches = list()\n",
    "pages = list()\n",
    "for i in range(3):\n",
    "    m = [m for m in SequenceMatcher(None, contexts[i].page_content, reference[\"source\"][0]).get_matching_blocks() if m.size > 15]\n",
    "    if m:\n",
    "        # m = contexts[i].page_content[m[0].a : m[-1].a + m[-1].size]\n",
    "        # matches.append(m)\n",
    "        for j in range(len(m)):\n",
    "            matches.append(contexts[i].page_content[m[j].a : m[j].a + m[j].size])\n",
    "            pages.append(contexts[i].metadata[\"page\"])\n",
    "\n",
    "df_sources = pd.DataFrame({\"match\": matches, \"page\": pages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def dict_to_string(input_dict):\n",
    "    json_string = json.dumps(input_dict, indent=2)  # indent for pretty formatting (optional)\n",
    "    return json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex_python.converter import CurrencyRates\n",
    "\n",
    "def get_cost(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        prompt_formatted=prompt.format(context=contexts_formatter(contexts), question=question, chat_history=memory.buffer_as_messages),\n",
    "        output_from_llm=dict_to_string(res)\n",
    "):\n",
    "    curr = CurrencyRates()\n",
    "    encoder = tiktoken.encoding_for_model(model)\n",
    "    input_tokens_used = len(encoder.encode(prompt_formatted)) + 7 # Jaga-jaga\n",
    "    output_tokens_used = len(encoder.encode(output_from_llm))\n",
    "    total_token = input_tokens_used + output_tokens_used\n",
    "\n",
    "    input_price = round((0.0015/1000) * input_tokens_used, 8)\n",
    "    output_price = round((0.002/1000) * output_tokens_used, 8)\n",
    "    total_price_usd = round(input_price + output_price, 8)\n",
    "    total_price_idr = curr.convert('USD', 'IDR', total_price_usd)\n",
    "\n",
    "\n",
    "    return f\"\"\"Tokens Used: {total_token}\n",
    "        Prompt Tokens: {input_tokens_used}\n",
    "        Completion Tokens: {output_tokens_used}\n",
    "    Total Cost (USD): ${total_price_usd}\n",
    "    Total Cost (IDR): Rp{total_price_idr}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlighting Sources\n",
    "def get_matches(source, contexts=contexts, k=3):\n",
    "    for i in range(k):\n",
    "        idx_awal = contexts[i].page_content.find(source)\n",
    "        if idx_awal != -1:\n",
    "            idx_akhir = contexts[i].page_content[idx_awal:].find(\".\")\n",
    "            idx_akhir += idx_awal\n",
    "        \n",
    "            match_ = contexts[i].page_content[idx_awal:idx_akhir]\n",
    "            if len(match_) > 10:\n",
    "                page_num = contexts[i].metadata[\"page\"]\n",
    "                return contexts[i].page_content[idx_awal:idx_akhir], page_num\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def highlight_pdf(path, match_, page_num, output_path):\n",
    "    pdf = fitz.open(path)\n",
    "    page = pdf[page_num]\n",
    "\n",
    "    matches = page.search_for(match_.replace(\"-\\n\", \"\"))\n",
    "\n",
    "    for m in matches:\n",
    "        page.add_highlight_annot(m)\n",
    "\n",
    "    pdf.save(output_path)\n",
    "    pdf.close()\n",
    "\n",
    "def highlight_pdf_v2(path, df_sources, output_path):\n",
    "    for i in range(len(df_sources)):\n",
    "        if i == 0:\n",
    "            pdf = fitz.open(path)\n",
    "            page = pdf[df_sources[\"page\"].loc[i]]\n",
    "\n",
    "            matches = page.search_for(df_sources[\"match\"].loc[i].replace(\"-\\n\", \"\"))\n",
    "\n",
    "            for m in matches:\n",
    "                page.add_highlight_annot(m)\n",
    "\n",
    "        else:\n",
    "            pdf = fitz.open(path)\n",
    "            page = pdf[df_sources[\"page\"].loc[i]]\n",
    "\n",
    "            matches = page.search_for(df_sources[\"match\"].loc[i].replace(\"-\\n\", \"\"))\n",
    "\n",
    "            for m in matches:\n",
    "                page.add_highlight_annot(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get references\n",
    "def get_reference(docs, in_text_citation):\n",
    "    model = OpenAI()\n",
    "\n",
    "    get_citation_template = \"\"\"From the reference list below, rewrite the specified references!\n",
    "    References:\n",
    "    {references}\n",
    "\n",
    "    Please rewrite the references related to the following numbers:\n",
    "    {in_text_citation}\n",
    "    \"\"\"\n",
    "\n",
    "    GET_CITATION_PROMPT = PromptTemplate.from_template(get_citation_template)\n",
    "\n",
    "    get_reference_chain = chain = GET_CITATION_PROMPT | model\n",
    "\n",
    "    for page_number in range(len(docs)):\n",
    "        page = docs[page_number]\n",
    "        text = page.page_content\n",
    "\n",
    "        if \"References\" in text or \"REFERENCES\" in text:\n",
    "            references_text = text.split(\"References\")[1] if \"References\" in text else text.split(\"REFERENCES\")[1]\n",
    "\n",
    "    result = get_reference_chain.invoke({\"references\": references_text, \"in_text_citation\":in_text_citation})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if res[\"source\"]:\n",
    "    match_, page_num = get_matches(res[\"source\"][:15], contexts)\n",
    "\n",
    "    if match_:\n",
    "        highlight_pdf(\"pdfs/2309.13963.pdf\", match_, page_num, \"pdfs/highlighted/high_pdf.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20] J. Li, D. Li, S. Savarese, and S. Hoi, “BLIP-2: Bootstrapping Language-Image Pre-Training with Frozen Image Encoders and Large Language Models,” in Proc. ICML , Vienna, 2023.\n"
     ]
    }
   ],
   "source": [
    "if reference[\"in-text citation\"]:\n",
    "    ref_result = get_reference(docs, reference[\"in-text citation\"]).strip()\n",
    "print(ref_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
